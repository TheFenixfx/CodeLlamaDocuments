# CodeLlama Documents
Load Documents into a VectorStore and get Context about it. The more paramaters and better embedings used, better results. 

Jupiter Notebook Version
Langchain and local model compatibility through LlamaCPP ( Langchain implementation )

Require : 
Nvidia GPU +8GB
llamacpp, chromadb and Langchain
